% !TEX encoding = UTF-8 Unicode
% Capítulo 4 - Implementación
\chapter[Implementación]{Implementación}

En este capítulo se introducirán los detalles más relevantes del desarrollo de una nueva
herramienta para la minería de procesos, \pachtool, realizada como parte de esta tesina. 

Además, se presentarán y analizarán diferentes resultados experimentales 
de utilizar \pachtool como una instancia del~\autoref{algo} y además
de utilizarlo como una herramienta de
simplificación y generalización sobre un cierto modelo dado. 

\section{PacH}
\label{sec:4.pach}

Para el desarrollo de esta tesina, el~\autoref{algo} ha sido implementado íntegramente 
como una herramienta de linea de comando llamada \pachtool~\cite{pach}.

La herramienta se encuentra desarrollada en Python y ha sido satisfactoriamente probada en Linux y MAC;
el código fuente se encuentra disponible bajo licencia BSD en
\begin{center}\url{http://github.com/lucionardelli/PacH}\end{center}

\pachtool maneja los estándar para logs (XES) y redes de Petri (PNML) y puede funcionar como 
una herramienta de descubrimiento de procesos a partir de logs, generalización y eliminación
o simplemente como una herramienta de post procesamiento de modelos existentes.

Una lista exhaustiva de comandos y opciones se encuentra disponible en el sitio de la herramienta.

%Assume that we have a log file of positive traces \verb!log.xes! and a log file of forbidden traces \verb!neg_log.xes!, we instruct \pachtool to do supervised process discovery with the following command:
%\begin{verbatim}
%./pach.py log.xes
%  [--negative neg_log.xes]
%  [--sampling [<number of samplings>] [<sampling size>]]
%  [--projection [<max group size>] [<connected model>]]
%  [--smt-matrix [<timeout>]]
%  [--smt-iter [<timeout>]]
%\end{verbatim}	
%
%By default the tool only considers positive traces and generates the net obtained from the polyhedron covering all positive points. Negative information can be used with the option \verb!--negative!. Options \verb!--projection! and \verb!--sampling! can be used to project and sample; \verb!--smt-matrix! and \verb!--smt-iter! instructs \pachtool to simplify the matrix or the half-spaces respectively. \fxnote{Lucio}

%It is worth noticing that by the non-determinism of the sampling selection, the information used for the projection and the solution obtained by the SMT-solver, \pachtool may generate different nets when the tool is called several times with the same inputs.

\subsection*{Instalación}
Para utilizar \pachtool, se deben cumplir con algunos requerimientos. Para esto, 
nos basamos en \texttt{pip}~\cite{pip}, un manejador de paquetes de Python. Con esto,
simplemente se debe correr el siguiente comando para obtener todos los
paquetes necesarios:

\begin{figure}[H]
    \centering
    \begin{Verbatim}[frame=single,fontsize=\scriptsize]
        pip install -r requirements.txt
    \end{Verbatim}
    \caption{Instalación rápida de dependencias para \pachtool.}
    \label{ej:pip_install}
\end{figure}

La lista completa de requerimientos, con la versión especificada para cada paquete,
puede verse el archivo \texttt{requirements.txt}.

\section{Estructura General}
\label{sec:4.general}

Ideada inicialmente como una herramienta de experimentación,
que permitiese validar los nuevos resultados teóricos propuestos, su crecimiento ha ido de la mano
de las necesidades, finalizando con una implementación completa en Python del~\autoref{algo},
con posibilidad de realizar el proceso de simplificación y generalización sobre cualquier modelo.
Además, se cuenta con la posibilidad de comparar los distintos métodos de simplificación
implementados gracias a una serie de objetos abstractos que permiten medir tiempos de 
ejecución de cada etapa independientemente de los métodos utilizados, 
generando una salida uniforme.

\pachtool cuenta con diferentes módulos, cada uno con interfaces específicas que pueden utilizarse de manera
independiente y también se provee de una interfaz única de centralización y cómputo general para 
facilitar la interacción entre ellas, permitiendo la ejecución del algoritmo 
de descubrimiento, simplificación y generalización completo.

A continuación se realizará un resumen de las funcionalidades que presenta \pachtool;
una guía rápida de uso, detallando los argumentos y opciones de cada componente,
puede encontrarse en el repositorio mencionado.

\subsection{Interpretación de archivos}
\label{sec:4.parsing}

Se ha implementado mediante el patrón de diseño \textit{strategy} por lo que permite
de manera sencilla extender los algoritmos de parseo para nuevos formatos.
Actualmente, utilizada como herramienta de minería de procesos, permite interpretar 
logs en el formato estándar, XES (\autoref{sec:2.logs}) o en formato de texto (con eventos
separados por espacios y fin de línea para separar trazas).

Por otro lado, si se utiliza \pachtool como una herramienta de post procesamiento, 
permite interpretar cualquier modelo que se encuentre representado
como una red de Petri mediante el estándar PNML.

\subsection{Poliedros convexos}
\label{sec:4.convex_polyhedra}

%Basándose en la relación existente entre redes de Petri y la $H$-representación de un poliedro presentada en~\autoref{sec:2.discovery},

\subsubsection{Poliedro convexo}
\label{sec:4.qhull}

Uno de los procesos más costosos del \autoref{algo} consiste en obtener el poliedro convexo que contenga 
el conjunto de vectores Parikh de un log, i.e. linea \ref{algo:lineqhull} del algoritmo.
El procedimiento correspondiente a \textsc{ConvexHull} ha sido implementado utilizando el paquete Python \emph{pyhull}~\cite{pyhull}, 
un wrapper para Python del \qhulltool~\cite{Barber96}, y mediante las técnicas de alto nivel para manejo de 
logs de gran dimensión, introducidas en~\cite{CarmonaC14}, sampling y projection.

Mediante el paquete pyqhull se obtiene la $H$-representación a coeficientes en $\mathbb{R}$ correspondiente a la cápsula convexa
para los puntos. Como se vio en la~\autoref{sec:2.discovery polyhedra}, se requiere el $Z$-poliedro,
por lo cual se utilizan operadores de expansión que permiten obtener una \dquote{sobre-representación} a coeficientes en $\mathbb{Z}$.

\subsubsection{SMT-poliedro}
\label{sec:4.smthull}

Adicionalmente \pachtool implementa la posibilidad de codificar el problema de obtener un 
poliedro convexo que represente el comportamiento de un cierto log y que no permita ninguna de los 
de las trazas correspondientes a un log negativo mediante el uso de SMT-solvers.

Para poder utilizar este enfoque, se busca minimizar la cantidad de semiespacios utilizados
en la $H$-representación de un cierto poliedro que contenga el conjunto de vectores Parikh
del log y no contemple ninguno de los puntos negativos.
Este método posee una limitante inevitable y es que el uso de información negativa es requerido.
Si no se utilizan puntos prohibidos, limitando el poliedro de manera que no los contenga,
la solución obtenida mediante este método es demasiado general, degradando por completo el modelo.
%Las soluciones de un SMT-solver suelen ser más rápidas si no se codifica la búsqueda de la solución óptima a un cierto problema,
%es por esto que en ocasiones es preferible un enfoque iterativo de resolución.
%Esta solución es la implementada: se busca la resolución al problema de forma iterativa, minimizando la cantidad de hiperplanos
%mediante los cuales se describe el poliedro que contiene el conjunto de puntos.

Es importante aclarar, que aún cuando los poliedros que se obtienen mediante este procedimiento 
representan el conjunto trazas del log de manera más eficaz que lo obtenidos mediante el 
enfoque usual (i.e. el presentado en ~\autoref{sec:4.qhull}), no lo hacen de manera eficiente.
Para casos artificiales los resultados son auspiciosos, pero el tiempo de computo insumido 
resulta prohibitivo para el uso de SMT-poliedro en casos de uso reales.

\subsection{Redes de Petri}
\label{sec:4.petri}

\pachtool implementa una representación interna para redes de Petri que permite trabajar con ellas de manera sencilla.
Mediante esta representación se codifica la biyección existente entre una red de Petri y la $H$-representación 
correspondiente a un poliedro utilizando la relación presentada en~\autoref{sec:2.discovery}, permitiendo
moverse entre las representaciones en ambas direcciones de manera transparente.

Además se cuenta con la posibilidad de generación del archivo PNML\footnote{PNML consiste del estándar para representación
de redes de Petri, por sus siglas en inglés \textit{\textbf{P}etri \textbf{N}et \textbf{M}arkup \textbf{L}anguage}},
correspondiente a la solución simplificada del modelo obtenido.

\subsection{Generalización y simplificación}
\label{sec:4.simplification}

Los procedimientos de generalización y simplificación mediante SMT-solvers se encuentran implementados utilizando \texttt{Z3}~\cite{MouraB08}. 
La herramienta soporta la codificación SMT presentada en este trabajo (i.e. considerando la matriz extendida
como entrada del SMT-Solver) así como también admite la representación de~\cite{LeonCB15} (i.e. considerando los semiespacios uno a uno).

Ambos métodos de simplificación han sido codificados siguiendo dos enfoques diferentes soportados por \texttt{Z3}
siendo el enfoque a utilizar es una de las variables de configuración.
El primer enfoque consiste en codificar el problema como un problema de minimización, buscando 
el menor valor para la complejidad estructural del modelo.
El segundo enfoque por su parte, consiste en un proceso iterativo; en cada iteración se busca
una \dquote{mejor} solución (i.e. con menor complejidad estructural asociada) y se repite el proceso
hasta que no se pueda obtener tal solución.
En aquellos casos donde el primer enfoque termina, el resultado se obtiene más rápidamente y además 
suele ser más preciso comparado con el método iterativo. Sin embargo, el segundo enfoque
permite encontrar soluciones para casos en los cuales el primer enfoque falla.
Experimentalmente se recomienda utilizar el primer enfoque siempre que la cantidad de eventos del log
sea cercana a los 1200 eventos.

\section{Resultados experimentales}
\label{sec:4.experiments}

El enfoque descripto en~\autoref{chap:3}, ha sido evaluado en diferentes logs correspondientes
a ejemplos tanto artificiales como reales. Con el fin de comprobar la universalidad de la técnica presentada,
como se vio en~\autoref{sec:3.simplification}, se tomaron además modelos obtenidos mediante \textit{ILP miner}~\cite{WDHS08},
una herramienta para minería de procesos que obtiene también redes de Petri adecuadas.

La calidad de las redes obtenidas y simplificadas se evalúa mediante las técnicas en~\cite{AMCDA15} para medir
la precisión de un modelo y el enfoque en~\cite{BrouckeWVB14} para medir la generalización. El grado de
simplificación obtenido se obtiene según lo visto en~\autoref{sec:3.complexity}.
Cabe aclarar que dado que las técnicas de descubrimiento basadas en teoría de poliedros y la herramienta \textit{ILP miner}
generan modelos adecuados, el~\autoref{theo:fit} garantiza que todos los modelos generados y/o simplificados
mediante el enfoque de esta tesina serán también adecuados.

Los experimentos han sido ejecutados en una máquina de 16 núcleos, 2.3GHz y 24GB de memoria RAM y los tiempos
reportados se encuentran expresados en segundos.

\section{Descubrimiento supervisado de procesos}
\label{sec:4.supervised}

\input{tables/table_benchmarks}

La herramienta ha sido ejecutada en diferentes benckmarks, reportando datos relevantes como la 
complejidad esctructural del modelo obtenido, precisión, generalización y los tiempos en segundos 
empleados en por cada acción significativa del proceso (e.g. algoritmo de descubrimiento,
proceso de simplificación iterativo, generalización).
El conjunto de benchmarks (trazas positivas y negativas) puede verse en~\autoref{tab:bench}, donde se
indican el número de trazas en cada log $|\pmlog|$, el número de eventos $|E|$ (i.e. la suma de las 
longitudes de todas las trazas) y el número total de tipos diferentes de actividades $|T|$.
Las trazas negativas han sido generadas utilizando el algoritmo propuesto en~\cite{BrouckeWVB14},
concatenando cada traza con un postfijo aleatoria con longitud igual a la longitud original de la traza
negativa.

El conjunto completo de benchmarks, conjuntamente con la salida de las diferentes ejecuciones puede 
encontrarse en el repositorio de \pachtool.

En las tablas \ref{tab:pol_time} a~\ref{tab:pol_gen}, se encuentran los resultados para cada benchmark siguiendo
el siguiente formato:

\begin{itemize}
    \item \textbf{Poliedro:} corresponde al enfoque en ~\cite{CarmonaC14} sin aplicar simplificaciones.
    \item \textbf{Semiespacio:} corresponde al enfoque en \cite{CarmonaC14} aplicando simplificación mediante SMT-solvers según el enfoque de~\cite{LeonCB15}.
    \item \textbf{Matriz:} corresponde al enfoque en \cite{CarmonaC14} aplicando simplificación mediante SMT-solvers según lo 
      presentado en~\autoref{sec:3.gensimp}.
    \item \textbf{\textsc{Removal}:} corresponde al proceso \textsc{Removal} aplicado sobre el modelo base en la $1^{er}$ columna (Base) 
      y sobre el modelo obtenido luego de la simplificación en la $2^{da}$ columna (SMT).
      \todo[inline]{Esto no me gusta, pero la verdad es que no entiendo bien que representan las columnas de Removal, así que no se como parafresearlo...}
\end{itemize}

%%%For the SMT simplification, a timeout of 600 milliseconds has been set for the solver. The selection have been done based on previous experience of the authors using SMT solver; normally if no answer is obtained for encodings of this size after the selected time threshold, no answer will be obtained even if the timeout is completely removed.

%Loss resultados de las tablas a continuación consitutyen el resultado de una estrategia de validación basada en 10
%corridas diferentes

%%%To calculate all the results as presented in the tables below, we apply a ten-fold cross-validation strategy as follows: 
%%%for each input event log, we split the log in ten equally sized new logs. Next, models are discovered and simplified 
%%%over 
%%%nine out of ten sub-logs, leaving out the remaining 10 percent of traces, repeated ten times (e.g. in the first fold, a 
%%%net is discovered whilst leaving out log number 1, in the second fold, a net is discovered whilst leaving out log number 
%%%2, and so on). To obtain conformance checking results, each originally left-out sub-log in each of the ten folds is 
%%%replayed over the discovered net, hence obtaining out-of-fold results. At the end of the run, final values are obtained 
%%%by averaging the results over all folds. We highlight that, although such a cross-validated setup is estándar practice 
%%%in the field of machine learning, it is rarely encountered in process mining works, as such a setup is generally 
%%%considered unnecessary or undesirable due to the fact that most discovered models are used for descriptive, rather than 
%%%predictive means. Secondly, it can also be argued that applying a cross-validation pre-processing step is less 
%%%straightforward than with normal, traditional data mining setups, as the random sampling of traces from an event log 
%%%might create sub-logs which miss some key behavioral patterns, which are still contained in other sub-logs (and which were 
%%%contained in the original log), such as the inclusion of certain activities in the activity alphabet or the inclusion of 
%%%every binary relationship present in the original log. Since both discovery techniques included in our setup guarantees 
%%%the discovery of perfectly fitting nets (i.e. a fitness/recall value of 1), this provides us with a good means to 
%%%inspect the impact of the loss of behavior stemming from cutting the event log. Indeed, we have generally observed that 
%%%out-of-fold averaging of recall scores provides recall scores which are below one, though the decrease was minimal for 
%%%all 
%%%logs, indicating that the cross-validation setup applied is safe for further use and that the presented metrics below are valid for use and interpretation.
%%%


Los resultados relacionados al tiempo de ejecución de \pachtool se reportan en~\autoref{tab:pol_time}.
La primera columna muestra el tiempo para calcular la cápsula convexa; el resto de las columnas, informa
sobre el tiempo de la simplificación. Por ejemplo, el tiempo total de \pachtool como método de descubrimiento
mediante la simplificación SMT iterativo mediante información positiva corresponde a la suma de la $1^{er}$ y $2^{da}$ columnas.
%En general el procedimiento \textsc{Removal} corresponde a la operación que mayor tiempo consume, debido a que se
%debe iterar sobre todas los semiespacios y todos los puntos negativos verificando si se puede elimnar.
Dado que para las codificaciones SMT se utiliza un enfoque iterativo para obtener la solución óptima,
suelen ser necesarias varias iteraciones para la codificación de~\cite{LeonCB15} ($2^{da}$ y $4^{ta}$ columnas). 
Esto produce un gran aumento de los tiempos de la $2^{da}$ columna para las redes que tienen mayor cantidad de places
e.g. \bench[32]{A}, \bench[42]{A}, \bench{Svn}; este comportamiento no se repite para la $4^{ta}$ columna
debido a que el SMT-solver no es capaz de encontrar una solución (i.e. devuelve \emph{unsat}) y el método no debe computar todos los casos.
Por otro lado, al aplicar el enfoque presentado en esta tesina (columnas $3^{ra}$ y $5^{ta}$), los tiempos son 
generalmente bajos debido a que, luego de pocas iteraciones, se encuentra la solución óptima o no se encuentra solución.
Por su parte, puede verse como los benchmarks que presentan gran cantidad de places insumen más tiempo que el resto
en el cálculo del poliedro (i.e. correspondiente a la primer columna). Esto es debido al hecho de que se debe
utilizar la técnica de proyección de~\cite{CarmonaC14} y esto consume gran parte del tiempo computacional.

\input{tables/table_pol_time}
\input{tables/table_pol_simp}

En~\autoref{tab:pol_simp} se reportan los resultados en la simplicidad -o complejidad estructural
según lo visto en~\autoref{sec:3.complexity}- alcanzados. 
Puede verse que la primera columna muestra los valores más altos dado a que no se realiza simplificación.
Para todos los casos los mejores resultados (resaltados en negrita) se obtienen después de aplicar el 
procedimiento de \textsc{Removal} luego de aplicar la simplificación SMT iterativamente sobre cada semiespacio
con información positiva (es decir, el neto obtenido en la $2^{da}$ columna). 
Entre los resultados pueden observarse algunos casos en los cuales no fue posible obtener
un modelo simplificado (celdas con cruces). Esto ocurre al utilizar simplificación estructural 
sin información negativa o en el caso de la simplificación iterativa utilizando información negativa.
En el primer caso, esto se debe a que el SMT no puede obtener una solución mejor sin alcanzar el timeout impuesto
para fines experimentales (parámetro configurable del algoritmo). El segundo caso se debe a la naturaleza restrictiva
del proceso (como se explica en la~\autoref{fig:glob_encoding}); ya que se utilizan tantas trazas negativas como positivas,
cada simplificación posee gran cantidad de restricciones haciendo imposible que se mejore.


Las tablas~\ref{tab:pol_prec} y \ref{tab:pol_gen} muestran, respectivamente, la precisión y generalización de las redes
obtenidas mediante \pachtool. De la~\autoref{tab:pol_prec}, se puede observar que 
aplicar simplificación SMT iterativamente sólo con información positiva disminuye en gran medida la precisión de las
redes (e.g. \bench{option}, \bench{ConfDimB}) . Sin embargo, esto no ocurre si se considera 
información negativa; en la mayoría de los casos la simplificación estructural no logra reducir o generalizar.
En términos de generalización (ver ~\autoref{tab:pol_gen}) los valores mínimos y máximos están muy 
cerca. Para la mitad de los benchmarks, \pachtool sin simplificación ya obtiene los mejores resultados;
para el resto de casos, se obtiene el mejor resultado luego de aplicar el proceso de \textsc{Removal}.

\input{tables/table_pol_prec}

Para resumir, las columnas $5^{ta}$ y $6^{ta}$ de las tablas ~\ref{tab:pol_prec} y~\ref{tab:pol_gen} muestran que 
la precisión y la generalización son similares al utilizar la simplificación estructural utilizando información 
negativa o el procedimiento \textsc{Removal}; 
Si tenemos en cuenta la relación costo/beneficio de las métricas de calidad, este resultado,
en combinación con los resultados de la~\autoref{tab:pol_simp}
sugieren que el proceso óptimo consiste en aplicar el método presentado en esta tesina (i.e. simplificación de la matriz 
extendida del sistema) seguido del procedimiento de \textsc{Removal}.

\input{tables/table_pol_gen}

\section{Resumen del capítulo}
\label{sec:4.resumen}

En este capítulo se introdujo la herramienta \pachtool, realizada como parte de esta tesina. Se realizó un breve
resumen de las cualidades que presenta así como algunos detalles referentes a su implementación.

A su vez, se presentaron los resultados experimentales de aplicar \pachtool tanto como un algoritmo de 
descubrimiento, generalización y simplificación (i.e. una implementación del ~\autoref{algo}) así como 
utilizada como herramienta de post procesamiento sobre modelos obtenidos mediante otra herramienta de 
minería de procesos, \textit{ILP miner}.

Por último se realizó un análisis de los resultados obtenidos y se pudo concluir que la herramienta presentada
es capaz de generar, dado un modelo adecuado de un cierto sistema, un nuevo modelo con una menor complejidad estructural
sin con esto degenerar la precisión del mismo.

